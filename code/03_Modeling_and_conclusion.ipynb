{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3 - Web APIs & NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "- [Library & dataset imports](#Importing-libraries-and-clean-dataset)\n",
    "- [Data Dictionary](#Data-Dictionary)\n",
    "- [Editing stop words](#Adding-words-to-scikit-learn's-CountVectorizer's-stop-list)\n",
    "- [Baseline Accuracy](#Baseline-Accuracy)\n",
    "- [Modeling with Multinominal Naive Bayse (Warning!: n_jobs have been set to -1)](#Multinominal-Naive-Bayes-model)\n",
    "- [Modeling with KNearestNeighbors (Warning!: n_jobs have been set to -1)](#KNearestNeighbors-model)\n",
    "- [Modeling with Random Forest (Warning!: n_jobs have been set to -1)](#Random-Forest)\n",
    "- [Score Summary](#Score-Summary)\n",
    "- [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries and clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "# this setting widens how many characters pandas will display in a column:\n",
    "pd.options.display.max_colwidth = 400\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.read_csv(\"../datasets/final_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1992, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "\n",
    "\n",
    "\n",
    "|Column|Dataset|Data Description|\n",
    "|---|---|---|\n",
    "|subreddit|final_data.csv|Subreddit of post taken from with (0: Vegan, 1: Keto)|\n",
    "|lemm_words|final_data.csv|Lemmertized words of cleaned title and posts concatenated|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>lemm_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>1</td>\n",
       "      <td>third time doing keto a a type 2 diabetic i'm worried that it's too late i wa first diagnosed with type 2 diabetes about 4 year ago at the time i wa really afraid so i began researching the internet for the best diet to treat this disease which is when i first learned about keto i wa so impressed with the science behind it that i decided to give it a try and it worked it only took a day or two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>1</td>\n",
       "      <td>keto while maintaining a calorie deficit doe anyone have tip on how to do low cal keto i feel like every recipe i find is either all bacon or all cheese which are both super high in calorie where are the resource for low cal healthy keto been doing keto for 1 5 month with no mess ups now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>1</td>\n",
       "      <td>stalled weight loss hi i have lost ten pound but i have been stuck at the 230 pound mark for about 5 day i am definitely in a caloric deficit so idk what s going on doe anyone know why i might be stalling despite the large calorie defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>1</td>\n",
       "      <td>raw spinach and oxylates are they really that bad i like to use raw spinach a a base for a lot of sauce based dish almost like one would normally use rice or pasta or a tortilla it's super convenient low in carbs and seems like a fairly nutrient dense food a opposed to shirataki rice for example which ha nothing in it besides fiber and water however i m concerned a little bit about oxylates an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>1</td>\n",
       "      <td>need some lunch and dinner idea hey guy i ve been eating really really basic food lately such a baked chicken beef and broccoli egg egg more egg sausage bacon taco with beef taco with chicken stuff like that but it s alllll sooo basic and day 7 is really starting to feel rough because i open my fridge and i just think blah i m no chef otherwise i would have much more diverse option so i need y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit  \\\n",
       "1987          1   \n",
       "1988          1   \n",
       "1989          1   \n",
       "1990          1   \n",
       "1991          1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                           lemm_words  \n",
       "1987  third time doing keto a a type 2 diabetic i'm worried that it's too late i wa first diagnosed with type 2 diabetes about 4 year ago at the time i wa really afraid so i began researching the internet for the best diet to treat this disease which is when i first learned about keto i wa so impressed with the science behind it that i decided to give it a try and it worked it only took a day or two...  \n",
       "1988                                                                                                                keto while maintaining a calorie deficit doe anyone have tip on how to do low cal keto i feel like every recipe i find is either all bacon or all cheese which are both super high in calorie where are the resource for low cal healthy keto been doing keto for 1 5 month with no mess ups now   \n",
       "1989                                                                                                                                                                   stalled weight loss hi i have lost ten pound but i have been stuck at the 230 pound mark for about 5 day i am definitely in a caloric deficit so idk what s going on doe anyone know why i might be stalling despite the large calorie defect   \n",
       "1990  raw spinach and oxylates are they really that bad i like to use raw spinach a a base for a lot of sauce based dish almost like one would normally use rice or pasta or a tortilla it's super convenient low in carbs and seems like a fairly nutrient dense food a opposed to shirataki rice for example which ha nothing in it besides fiber and water however i m concerned a little bit about oxylates an...  \n",
       "1991  need some lunch and dinner idea hey guy i ve been eating really really basic food lately such a baked chicken beef and broccoli egg egg more egg sausage bacon taco with beef taco with chicken stuff like that but it s alllll sooo basic and day 7 is really starting to feel rough because i open my fridge and i just think blah i m no chef otherwise i would have much more diverse option so i need y...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1992 entries, 0 to 1991\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   subreddit   1992 non-null   int64 \n",
      " 1   lemm_words  1992 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 31.2+ KB\n"
     ]
    }
   ],
   "source": [
    "final_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding words to scikit learn's CountVectorizer's stop list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://stackoverflow.com/questions/24386489/adding-words-to-scikit-learns-countvectorizers-stop-list/24386751\n",
    "\n",
    "my_stop_words = ['https', 've', 'don', 'doesn', 'getting', 'going', 'got', 'ha', 'isn', 'wa','amp','i m']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(my_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembling the predictor(X) and target (y)\n",
    "X = final_data['lemm_words']\n",
    "y = final_data['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a train test split with stratitfy set to the distribution of target. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.502008\n",
       "0    0.497992\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline accuracy = 0.502008\n",
    "\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(predict,model,cmaps='YlOrBr'):\n",
    "    '''\n",
    "    Input is what we want to predict and the model we are using\n",
    "    Output is a confusion matrix and the scores\n",
    "    '''\n",
    "    preds = model.predict(predict)\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test,preds).ravel()\n",
    "    plot_confusion_matrix(model, predict, y_test, cmap=cmaps, values_format = 'd');\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(f\"True Positives: {tp}\\n\" )\n",
    "    print(f\"Accuracy: {(tp+tn)/cm.sum()}\")\n",
    "    print(f\"Misclassification: {1-((tp+tn)/cm.sum())}\")\n",
    "    print(f\"Precision: {tp/(tp+fp)}\")\n",
    "    print(f\"Sensitivity: {tp/(tp+fn)}\")\n",
    "    print(f\"Specificity: {tn/(tn+fp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinominal Naive Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a statistical classification technique based on Bayes Theorem. It is one of the simplest supervised learning algorithms. Naive Bayes classifier is the fast, accurate and reliable algorithm. Naive Bayes classifiers have high accuracy and speed on large datasets.\n",
    "Naive Bayes classifier calculates the probability of an event in the following steps:\n",
    "\n",
    "- Step 1: Calculate the prior probability for given class labels\n",
    "- Step 2: Find conditional probability with each attribute for each class\n",
    "- Step 3: Multiply same class conditional probability.\n",
    "- Step 4: Multiply prior probability with step 3 probability \n",
    "- Step 5: See which class has a higher probability, given the input belongs to the higher probability class.<sup>5</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CountVectorizer with Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pipeline with countvectorizer as the transformer and multinomialnb as the estimator\n",
    "pipe_cvec_nb = Pipeline([\n",
    "    ('cvec',CountVectorizer()),\n",
    "    ('nb',MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [400,800,1200,1600,2000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__stop_words': [None,stop_words],\n",
    "    'cvec__ngram_range': [(1,1), (1,2),(1,3)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cvec_nb = GridSearchCV(pipe_cvec_nb, # Pipeline that we created\n",
    "                 param_grid = pipe_params, # using the customized parameters that we want to search over\n",
    "                 cv = 5, # 5-fold cross-validation.\n",
    "                 verbose =1,\n",
    "                         n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   46.9s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'cvec__max_df': [0.9, 0.95],\n",
       "                         'cvec__max_features': [400, 800, 1200, 1600, 2000],\n",
       "                         'cvec__min_df': [2, 3],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       "                         'cvec__stop_words': [None,\n",
       "                                              frozenset({'a', 'about', 'above',\n",
       "                                                         'across', 'after',\n",
       "                                                         'afterwards', 'again',\n",
       "                                                         'against', 'all',\n",
       "                                                         'almost', 'alone',\n",
       "                                                         'along', 'already',\n",
       "                                                         'also', 'although',\n",
       "                                                         'always', 'am',\n",
       "                                                         'among', 'amongst',\n",
       "                                                         'amoungst', 'amount',\n",
       "                                                         'amp', 'an', 'and',\n",
       "                                                         'another', 'any',\n",
       "                                                         'anyhow', 'anyone',\n",
       "                                                         'anything', 'anyway', ...})]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cvec_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.964526048798007"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cvec_nb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9692101740294511"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training score\n",
    "gs_cvec_nb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9598393574297188"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test score\n",
    "gs_cvec_nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_cvec__max_df</th>\n",
       "      <th>param_cvec__max_features</th>\n",
       "      <th>param_cvec__min_df</th>\n",
       "      <th>param_cvec__ngram_range</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1600</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.964526</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1600</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.964526</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1600</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.963188</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1600</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.963188</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1600</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.963184</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.947790</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9</td>\n",
       "      <td>400</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.947786</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.95</td>\n",
       "      <td>800</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.947783</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.9</td>\n",
       "      <td>800</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.947783</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.9</td>\n",
       "      <td>800</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.947783</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    param_cvec__max_df param_cvec__max_features param_cvec__min_df  \\\n",
       "99                0.95                     1600                  2   \n",
       "39                 0.9                     1600                  2   \n",
       "105               0.95                     1600                  3   \n",
       "45                 0.9                     1600                  3   \n",
       "107               0.95                     1600                  3   \n",
       "..                 ...                      ...                ...   \n",
       "52                 0.9                     2000                  2   \n",
       "0                  0.9                      400                  2   \n",
       "80                0.95                      800                  3   \n",
       "20                 0.9                      800                  3   \n",
       "14                 0.9                      800                  2   \n",
       "\n",
       "    param_cvec__ngram_range  mean_test_score  rank_test_score  \n",
       "99                   (1, 2)         0.964526                1  \n",
       "39                   (1, 2)         0.964526                1  \n",
       "105                  (1, 2)         0.963188                3  \n",
       "45                   (1, 2)         0.963188                3  \n",
       "107                  (1, 3)         0.963184                5  \n",
       "..                      ...              ...              ...  \n",
       "52                   (1, 3)         0.947790              115  \n",
       "0                    (1, 1)         0.947786              117  \n",
       "80                   (1, 2)         0.947783              118  \n",
       "20                   (1, 2)         0.947783              118  \n",
       "14                   (1, 2)         0.947783              118  \n",
       "\n",
       "[120 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a dataframe to show the best rank score from the different params\n",
    "score1_df = pd.DataFrame(gs_cvec_nb.cv_results_).sort_values('rank_test_score')\n",
    "\n",
    "score1_df = score1_df[['param_cvec__max_df', 'param_cvec__max_features', 'param_cvec__min_df',\n",
    "       'param_cvec__ngram_range', 'mean_test_score','rank_test_score']]\n",
    "score1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the results of GridSearchCV with customized cvec parameters.\n",
    "- param_cvec_max_df: This parameter ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold. Using the first ranking model,  0.95 would represent 95%. Meaning to say if a word have a frequency above 95%, it would be ignored.\n",
    "\n",
    "- param_cvec_max_features: This parameter only consider the top\n",
    "    max_features ordered by term frequency across the corpus. Using the first ranking model as an example, 1600 would mean the top 1600 in terms of frequency.\n",
    "    \n",
    "- param_cvec_min_df: This parameter ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. Using the first ranking model, 2 would represent any word that appear less than twice will be ignored as well.\n",
    "\n",
    "- param_cvec_ngram_range: This parameter determines what  ùëõ -grams should be considered as features. Using the first ranking model as an example, (1,2) means it will capture every 1-gram and every 2-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 237\n",
      "False Positives: 11\n",
      "False Negatives: 9\n",
      "True Positives: 241\n",
      "\n",
      "Accuracy: 0.9598393574297188\n",
      "Misclassification: 0.04016064257028118\n",
      "Precision: 0.9563492063492064\n",
      "Sensitivity: 0.964\n",
      "Specificity: 0.9556451612903226\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_matrix(X_test,gs_cvec_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Our main goals/concerns are to:\n",
    "\n",
    "1. High accuracy of classification\n",
    "2. Increase the True Negative count.\n",
    "3. Reduce False Negative (Type II Error).\n",
    "4. Increase Specificity score.\n",
    "5. Increase Sensitivity score.\n",
    "\n",
    "The reason is because type II Error would mean that a post from the subreddit 'Keto' is being classified as 'Vegan'.\n",
    "Ethical vegans strongly believe that all creatures have the right to life and freedom. Therefore, they oppose ending a conscious being's life simply to consume its flesh, drink its milk, or wear its skin ‚Äî especially because alternatives are available. Whereas Keto diet consist of about 75% fat, 10-30% protein and no more than 5% or 20 to 50 grams of carbs per day. Focus on high-fat, low-carb foods like **eggs, meats, dairy** and low-carb vegetables, as well as sugar-free beverages. Therefore, it would be disastrous to classify a Keto post as Vegan. \n",
    "\n",
    "The score of this model is:\n",
    "- Accuracy: 0.9598\n",
    "- True Negative: 237\n",
    "- False Negative: 9\n",
    "- Specificity: 0.9556\n",
    "- Sensivitity: 0.964\n",
    "\n",
    "The training and test accuracy scores for this model are 96.92%, 95.98% respectively indicating that the model is well fitted as the scores are fairly similar, the false negative count indicates that 9 keto posts are wrongly classified as vegan and true negative count indicates the number of vegan post predicted correctly. Specificity score is at 95.56% indicating the accuracy of predicted vegan posts that were correctly identified. Sensitivity score is at 96.4%, representing the number of Keto posts being indentified correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using tf-idf vectorizer with Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pipeline with tf-idf vectorizer as the transformer and multinomialnb as the estimator\n",
    "pipe_tfid_nb = Pipeline([\n",
    "    ('tfid',TfidfVectorizer()),\n",
    "    ('nbm',MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params_tfid_nb = {\n",
    "    'tfid__max_features': [400,800,1200,1600,2000],\n",
    "    'tfid__min_df': [2, 3],\n",
    "    'tfid__max_df': [.9, .95],\n",
    "    'tfid__stop_words': [None,stop_words],\n",
    "    'tfid__ngram_range': [(1,1), (1,2),(1,3)]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tfid_nb = GridSearchCV(pipe_tfid_nb, # Pipeline that we created\n",
    "                 param_grid = pipe_params_tfid_nb, # using the customized params that we want to search over\n",
    "                 cv = 5, # 5-fold cross-validation.\n",
    "                 verbose =1,\n",
    "                         n_jobs=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   47.5s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tfid', TfidfVectorizer()),\n",
       "                                       ('nbm', MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'tfid__max_df': [0.9, 0.95],\n",
       "                         'tfid__max_features': [400, 800, 1200, 1600, 2000],\n",
       "                         'tfid__min_df': [2, 3],\n",
       "                         'tfid__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       "                         'tfid__stop_words': [None,\n",
       "                                              frozenset({'a', 'about', 'above',\n",
       "                                                         'across', 'after',\n",
       "                                                         'afterwards', 'again',\n",
       "                                                         'against', 'all',\n",
       "                                                         'almost', 'alone',\n",
       "                                                         'along', 'already',\n",
       "                                                         'also', 'although',\n",
       "                                                         'always', 'am',\n",
       "                                                         'among', 'amongst',\n",
       "                                                         'amoungst', 'amount',\n",
       "                                                         'amp', 'an', 'and',\n",
       "                                                         'another', 'any',\n",
       "                                                         'anyhow', 'anyone',\n",
       "                                                         'anything', 'anyway', ...})]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tfid_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9544881147448991"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tfid_nb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9678714859437751"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tfid_nb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9538152610441767"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tfid_nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfid__max_df': 0.9,\n",
       " 'tfid__max_features': 1600,\n",
       " 'tfid__min_df': 2,\n",
       " 'tfid__ngram_range': (1, 2),\n",
       " 'tfid__stop_words': frozenset({'a',\n",
       "            'about',\n",
       "            'above',\n",
       "            'across',\n",
       "            'after',\n",
       "            'afterwards',\n",
       "            'again',\n",
       "            'against',\n",
       "            'all',\n",
       "            'almost',\n",
       "            'alone',\n",
       "            'along',\n",
       "            'already',\n",
       "            'also',\n",
       "            'although',\n",
       "            'always',\n",
       "            'am',\n",
       "            'among',\n",
       "            'amongst',\n",
       "            'amoungst',\n",
       "            'amount',\n",
       "            'amp',\n",
       "            'an',\n",
       "            'and',\n",
       "            'another',\n",
       "            'any',\n",
       "            'anyhow',\n",
       "            'anyone',\n",
       "            'anything',\n",
       "            'anyway',\n",
       "            'anywhere',\n",
       "            'are',\n",
       "            'around',\n",
       "            'as',\n",
       "            'at',\n",
       "            'back',\n",
       "            'be',\n",
       "            'became',\n",
       "            'because',\n",
       "            'become',\n",
       "            'becomes',\n",
       "            'becoming',\n",
       "            'been',\n",
       "            'before',\n",
       "            'beforehand',\n",
       "            'behind',\n",
       "            'being',\n",
       "            'below',\n",
       "            'beside',\n",
       "            'besides',\n",
       "            'between',\n",
       "            'beyond',\n",
       "            'bill',\n",
       "            'both',\n",
       "            'bottom',\n",
       "            'but',\n",
       "            'by',\n",
       "            'call',\n",
       "            'can',\n",
       "            'cannot',\n",
       "            'cant',\n",
       "            'co',\n",
       "            'con',\n",
       "            'could',\n",
       "            'couldnt',\n",
       "            'cry',\n",
       "            'de',\n",
       "            'describe',\n",
       "            'detail',\n",
       "            'do',\n",
       "            'doesn',\n",
       "            'don',\n",
       "            'done',\n",
       "            'down',\n",
       "            'due',\n",
       "            'during',\n",
       "            'each',\n",
       "            'eg',\n",
       "            'eight',\n",
       "            'either',\n",
       "            'eleven',\n",
       "            'else',\n",
       "            'elsewhere',\n",
       "            'empty',\n",
       "            'enough',\n",
       "            'etc',\n",
       "            'even',\n",
       "            'ever',\n",
       "            'every',\n",
       "            'everyone',\n",
       "            'everything',\n",
       "            'everywhere',\n",
       "            'except',\n",
       "            'few',\n",
       "            'fifteen',\n",
       "            'fifty',\n",
       "            'fill',\n",
       "            'find',\n",
       "            'fire',\n",
       "            'first',\n",
       "            'five',\n",
       "            'for',\n",
       "            'former',\n",
       "            'formerly',\n",
       "            'forty',\n",
       "            'found',\n",
       "            'four',\n",
       "            'from',\n",
       "            'front',\n",
       "            'full',\n",
       "            'further',\n",
       "            'get',\n",
       "            'getting',\n",
       "            'give',\n",
       "            'go',\n",
       "            'going',\n",
       "            'got',\n",
       "            'ha',\n",
       "            'had',\n",
       "            'has',\n",
       "            'hasnt',\n",
       "            'have',\n",
       "            'he',\n",
       "            'hence',\n",
       "            'her',\n",
       "            'here',\n",
       "            'hereafter',\n",
       "            'hereby',\n",
       "            'herein',\n",
       "            'hereupon',\n",
       "            'hers',\n",
       "            'herself',\n",
       "            'him',\n",
       "            'himself',\n",
       "            'his',\n",
       "            'how',\n",
       "            'however',\n",
       "            'https',\n",
       "            'hundred',\n",
       "            'i',\n",
       "            'i m',\n",
       "            'ie',\n",
       "            'if',\n",
       "            'in',\n",
       "            'inc',\n",
       "            'indeed',\n",
       "            'interest',\n",
       "            'into',\n",
       "            'is',\n",
       "            'isn',\n",
       "            'it',\n",
       "            'its',\n",
       "            'itself',\n",
       "            'keep',\n",
       "            'last',\n",
       "            'latter',\n",
       "            'latterly',\n",
       "            'least',\n",
       "            'less',\n",
       "            'ltd',\n",
       "            'made',\n",
       "            'many',\n",
       "            'may',\n",
       "            'me',\n",
       "            'meanwhile',\n",
       "            'might',\n",
       "            'mill',\n",
       "            'mine',\n",
       "            'more',\n",
       "            'moreover',\n",
       "            'most',\n",
       "            'mostly',\n",
       "            'move',\n",
       "            'much',\n",
       "            'must',\n",
       "            'my',\n",
       "            'myself',\n",
       "            'name',\n",
       "            'namely',\n",
       "            'neither',\n",
       "            'never',\n",
       "            'nevertheless',\n",
       "            'next',\n",
       "            'nine',\n",
       "            'no',\n",
       "            'nobody',\n",
       "            'none',\n",
       "            'noone',\n",
       "            'nor',\n",
       "            'not',\n",
       "            'nothing',\n",
       "            'now',\n",
       "            'nowhere',\n",
       "            'of',\n",
       "            'off',\n",
       "            'often',\n",
       "            'on',\n",
       "            'once',\n",
       "            'one',\n",
       "            'only',\n",
       "            'onto',\n",
       "            'or',\n",
       "            'other',\n",
       "            'others',\n",
       "            'otherwise',\n",
       "            'our',\n",
       "            'ours',\n",
       "            'ourselves',\n",
       "            'out',\n",
       "            'over',\n",
       "            'own',\n",
       "            'part',\n",
       "            'per',\n",
       "            'perhaps',\n",
       "            'please',\n",
       "            'put',\n",
       "            'rather',\n",
       "            're',\n",
       "            'same',\n",
       "            'see',\n",
       "            'seem',\n",
       "            'seemed',\n",
       "            'seeming',\n",
       "            'seems',\n",
       "            'serious',\n",
       "            'several',\n",
       "            'she',\n",
       "            'should',\n",
       "            'show',\n",
       "            'side',\n",
       "            'since',\n",
       "            'sincere',\n",
       "            'six',\n",
       "            'sixty',\n",
       "            'so',\n",
       "            'some',\n",
       "            'somehow',\n",
       "            'someone',\n",
       "            'something',\n",
       "            'sometime',\n",
       "            'sometimes',\n",
       "            'somewhere',\n",
       "            'still',\n",
       "            'such',\n",
       "            'system',\n",
       "            'take',\n",
       "            'ten',\n",
       "            'than',\n",
       "            'that',\n",
       "            'the',\n",
       "            'their',\n",
       "            'them',\n",
       "            'themselves',\n",
       "            'then',\n",
       "            'thence',\n",
       "            'there',\n",
       "            'thereafter',\n",
       "            'thereby',\n",
       "            'therefore',\n",
       "            'therein',\n",
       "            'thereupon',\n",
       "            'these',\n",
       "            'they',\n",
       "            'thick',\n",
       "            'thin',\n",
       "            'third',\n",
       "            'this',\n",
       "            'those',\n",
       "            'though',\n",
       "            'three',\n",
       "            'through',\n",
       "            'throughout',\n",
       "            'thru',\n",
       "            'thus',\n",
       "            'to',\n",
       "            'together',\n",
       "            'too',\n",
       "            'top',\n",
       "            'toward',\n",
       "            'towards',\n",
       "            'twelve',\n",
       "            'twenty',\n",
       "            'two',\n",
       "            'un',\n",
       "            'under',\n",
       "            'until',\n",
       "            'up',\n",
       "            'upon',\n",
       "            'us',\n",
       "            've',\n",
       "            'very',\n",
       "            'via',\n",
       "            'wa',\n",
       "            'was',\n",
       "            'we',\n",
       "            'well',\n",
       "            'were',\n",
       "            'what',\n",
       "            'whatever',\n",
       "            'when',\n",
       "            'whence',\n",
       "            'whenever',\n",
       "            'where',\n",
       "            'whereafter',\n",
       "            'whereas',\n",
       "            'whereby',\n",
       "            'wherein',\n",
       "            'whereupon',\n",
       "            'wherever',\n",
       "            'whether',\n",
       "            'which',\n",
       "            'while',\n",
       "            'whither',\n",
       "            'who',\n",
       "            'whoever',\n",
       "            'whole',\n",
       "            'whom',\n",
       "            'whose',\n",
       "            'why',\n",
       "            'will',\n",
       "            'with',\n",
       "            'within',\n",
       "            'without',\n",
       "            'would',\n",
       "            'yet',\n",
       "            'you',\n",
       "            'your',\n",
       "            'yours',\n",
       "            'yourself',\n",
       "            'yourselves'})}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tfid_nb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_tfid__max_df</th>\n",
       "      <th>param_tfid__max_features</th>\n",
       "      <th>param_tfid__min_df</th>\n",
       "      <th>param_tfid__ngram_range</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1600</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.954488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1600</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.954488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1600</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.953819</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1600</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.953819</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1600</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.953817</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1600</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.936412</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1600</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.936412</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.95</td>\n",
       "      <td>800</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.936408</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1200</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.936408</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.9</td>\n",
       "      <td>800</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.935739</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    param_tfid__max_df param_tfid__max_features param_tfid__min_df  \\\n",
       "39                 0.9                     1600                  2   \n",
       "99                0.95                     1600                  2   \n",
       "107               0.95                     1600                  3   \n",
       "47                 0.9                     1600                  3   \n",
       "105               0.95                     1600                  3   \n",
       "..                 ...                      ...                ...   \n",
       "104               0.95                     1600                  3   \n",
       "44                 0.9                     1600                  3   \n",
       "80                0.95                      800                  3   \n",
       "92                0.95                     1200                  3   \n",
       "14                 0.9                      800                  2   \n",
       "\n",
       "    param_tfid__ngram_range  mean_test_score  rank_test_score  \n",
       "39                   (1, 2)         0.954488                1  \n",
       "99                   (1, 2)         0.954488                1  \n",
       "107                  (1, 3)         0.953819                3  \n",
       "47                   (1, 3)         0.953819                3  \n",
       "105                  (1, 2)         0.953817                5  \n",
       "..                      ...              ...              ...  \n",
       "104                  (1, 2)         0.936412              116  \n",
       "44                   (1, 2)         0.936412              116  \n",
       "80                   (1, 2)         0.936408              118  \n",
       "92                   (1, 2)         0.936408              119  \n",
       "14                   (1, 2)         0.935739              120  \n",
       "\n",
       "[120 rows x 6 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a dataframe to show the best rank score from the different params\n",
    "score2_df = pd.DataFrame(gs_tfid_nb.cv_results_).sort_values('rank_test_score')\n",
    "\n",
    "score2_df = score2_df[['param_tfid__max_df', 'param_tfid__max_features', 'param_tfid__min_df',\n",
    "       'param_tfid__ngram_range', 'mean_test_score','rank_test_score']]\n",
    "score2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 234\n",
      "False Positives: 14\n",
      "False Negatives: 9\n",
      "True Positives: 241\n",
      "\n",
      "Accuracy: 0.9538152610441767\n",
      "Misclassification: 0.04618473895582331\n",
      "Precision: 0.9450980392156862\n",
      "Sensitivity: 0.964\n",
      "Specificity: 0.9435483870967742\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_matrix(X_test,gs_tfid_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The scores of this model are:\n",
    "- Accuracy: 0.9538\n",
    "- True Negative: 234\n",
    "- False Negative: 9\n",
    "- Specificity: 0.9435\n",
    "- Sensitivity: 0.964\n",
    "\n",
    "The training and test accuracy scores for this model are 96.78%, 95.38% respectively indicating that the model is well fitted as both scores are fairly similar, the false negative count retain at 9 but true negative count has also drop to 234. This indicates that more Vegan post are wrongly classified as Keto. Specificity score also dropped to 94.35% as mentioned previously that true negative count has dropped and sensitivity retained at 96.4%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNearestNeighbors model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Count vectorizer with KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pipeline with countvectorizer as the transformer and K-NearestNeighbors as the estimator\n",
    "pipe_cvec_knn = Pipeline([\n",
    "    ('cvec',CountVectorizer()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params_cvec_knn = {\n",
    "    'cvec__max_features': [1200,1600,2000],\n",
    "    'cvec__min_df': [2],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__stop_words': [stop_words],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'knn__p': [1, 2], \n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__n_neighbors': [3, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cvec_knn = GridSearchCV(pipe_cvec_knn, # Pipeline that we creating\n",
    "                         param_grid = pipe_params_cvec_knn, # Using the customized params that we want to search over\n",
    "                         cv = 5, # 5-fold cross validation\n",
    "                         verbose=1,\n",
    "                           n_jobs = -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   33.7s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('knn', KNeighborsClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'cvec__max_df': [0.9, 0.95],\n",
       "                         'cvec__max_features': [1200, 1600, 2000],\n",
       "                         'cvec__min_df': [2],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'cvec__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                         'across', 'after',\n",
       "                                                         'afterwards', 'again',\n",
       "                                                         'against', 'all',\n",
       "                                                         'almost', 'alone',\n",
       "                                                         'along', 'already',\n",
       "                                                         'also', 'although',\n",
       "                                                         'always', 'am',\n",
       "                                                         'among', 'amongst',\n",
       "                                                         'amoungst', 'amount',\n",
       "                                                         'amp', 'an', 'and',\n",
       "                                                         'another', 'any',\n",
       "                                                         'anyhow', 'anyone',\n",
       "                                                         'anything', 'anyway', ...})],\n",
       "                         'knn__n_neighbors': [3, 5], 'knn__p': [1, 2],\n",
       "                         'knn__weights': ['uniform', 'distance']},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cvec_knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__max_features': 1200,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'cvec__stop_words': frozenset({'a',\n",
       "            'about',\n",
       "            'above',\n",
       "            'across',\n",
       "            'after',\n",
       "            'afterwards',\n",
       "            'again',\n",
       "            'against',\n",
       "            'all',\n",
       "            'almost',\n",
       "            'alone',\n",
       "            'along',\n",
       "            'already',\n",
       "            'also',\n",
       "            'although',\n",
       "            'always',\n",
       "            'am',\n",
       "            'among',\n",
       "            'amongst',\n",
       "            'amoungst',\n",
       "            'amount',\n",
       "            'amp',\n",
       "            'an',\n",
       "            'and',\n",
       "            'another',\n",
       "            'any',\n",
       "            'anyhow',\n",
       "            'anyone',\n",
       "            'anything',\n",
       "            'anyway',\n",
       "            'anywhere',\n",
       "            'are',\n",
       "            'around',\n",
       "            'as',\n",
       "            'at',\n",
       "            'back',\n",
       "            'be',\n",
       "            'became',\n",
       "            'because',\n",
       "            'become',\n",
       "            'becomes',\n",
       "            'becoming',\n",
       "            'been',\n",
       "            'before',\n",
       "            'beforehand',\n",
       "            'behind',\n",
       "            'being',\n",
       "            'below',\n",
       "            'beside',\n",
       "            'besides',\n",
       "            'between',\n",
       "            'beyond',\n",
       "            'bill',\n",
       "            'both',\n",
       "            'bottom',\n",
       "            'but',\n",
       "            'by',\n",
       "            'call',\n",
       "            'can',\n",
       "            'cannot',\n",
       "            'cant',\n",
       "            'co',\n",
       "            'con',\n",
       "            'could',\n",
       "            'couldnt',\n",
       "            'cry',\n",
       "            'de',\n",
       "            'describe',\n",
       "            'detail',\n",
       "            'do',\n",
       "            'doesn',\n",
       "            'don',\n",
       "            'done',\n",
       "            'down',\n",
       "            'due',\n",
       "            'during',\n",
       "            'each',\n",
       "            'eg',\n",
       "            'eight',\n",
       "            'either',\n",
       "            'eleven',\n",
       "            'else',\n",
       "            'elsewhere',\n",
       "            'empty',\n",
       "            'enough',\n",
       "            'etc',\n",
       "            'even',\n",
       "            'ever',\n",
       "            'every',\n",
       "            'everyone',\n",
       "            'everything',\n",
       "            'everywhere',\n",
       "            'except',\n",
       "            'few',\n",
       "            'fifteen',\n",
       "            'fifty',\n",
       "            'fill',\n",
       "            'find',\n",
       "            'fire',\n",
       "            'first',\n",
       "            'five',\n",
       "            'for',\n",
       "            'former',\n",
       "            'formerly',\n",
       "            'forty',\n",
       "            'found',\n",
       "            'four',\n",
       "            'from',\n",
       "            'front',\n",
       "            'full',\n",
       "            'further',\n",
       "            'get',\n",
       "            'getting',\n",
       "            'give',\n",
       "            'go',\n",
       "            'going',\n",
       "            'got',\n",
       "            'ha',\n",
       "            'had',\n",
       "            'has',\n",
       "            'hasnt',\n",
       "            'have',\n",
       "            'he',\n",
       "            'hence',\n",
       "            'her',\n",
       "            'here',\n",
       "            'hereafter',\n",
       "            'hereby',\n",
       "            'herein',\n",
       "            'hereupon',\n",
       "            'hers',\n",
       "            'herself',\n",
       "            'him',\n",
       "            'himself',\n",
       "            'his',\n",
       "            'how',\n",
       "            'however',\n",
       "            'https',\n",
       "            'hundred',\n",
       "            'i',\n",
       "            'i m',\n",
       "            'ie',\n",
       "            'if',\n",
       "            'in',\n",
       "            'inc',\n",
       "            'indeed',\n",
       "            'interest',\n",
       "            'into',\n",
       "            'is',\n",
       "            'isn',\n",
       "            'it',\n",
       "            'its',\n",
       "            'itself',\n",
       "            'keep',\n",
       "            'last',\n",
       "            'latter',\n",
       "            'latterly',\n",
       "            'least',\n",
       "            'less',\n",
       "            'ltd',\n",
       "            'made',\n",
       "            'many',\n",
       "            'may',\n",
       "            'me',\n",
       "            'meanwhile',\n",
       "            'might',\n",
       "            'mill',\n",
       "            'mine',\n",
       "            'more',\n",
       "            'moreover',\n",
       "            'most',\n",
       "            'mostly',\n",
       "            'move',\n",
       "            'much',\n",
       "            'must',\n",
       "            'my',\n",
       "            'myself',\n",
       "            'name',\n",
       "            'namely',\n",
       "            'neither',\n",
       "            'never',\n",
       "            'nevertheless',\n",
       "            'next',\n",
       "            'nine',\n",
       "            'no',\n",
       "            'nobody',\n",
       "            'none',\n",
       "            'noone',\n",
       "            'nor',\n",
       "            'not',\n",
       "            'nothing',\n",
       "            'now',\n",
       "            'nowhere',\n",
       "            'of',\n",
       "            'off',\n",
       "            'often',\n",
       "            'on',\n",
       "            'once',\n",
       "            'one',\n",
       "            'only',\n",
       "            'onto',\n",
       "            'or',\n",
       "            'other',\n",
       "            'others',\n",
       "            'otherwise',\n",
       "            'our',\n",
       "            'ours',\n",
       "            'ourselves',\n",
       "            'out',\n",
       "            'over',\n",
       "            'own',\n",
       "            'part',\n",
       "            'per',\n",
       "            'perhaps',\n",
       "            'please',\n",
       "            'put',\n",
       "            'rather',\n",
       "            're',\n",
       "            'same',\n",
       "            'see',\n",
       "            'seem',\n",
       "            'seemed',\n",
       "            'seeming',\n",
       "            'seems',\n",
       "            'serious',\n",
       "            'several',\n",
       "            'she',\n",
       "            'should',\n",
       "            'show',\n",
       "            'side',\n",
       "            'since',\n",
       "            'sincere',\n",
       "            'six',\n",
       "            'sixty',\n",
       "            'so',\n",
       "            'some',\n",
       "            'somehow',\n",
       "            'someone',\n",
       "            'something',\n",
       "            'sometime',\n",
       "            'sometimes',\n",
       "            'somewhere',\n",
       "            'still',\n",
       "            'such',\n",
       "            'system',\n",
       "            'take',\n",
       "            'ten',\n",
       "            'than',\n",
       "            'that',\n",
       "            'the',\n",
       "            'their',\n",
       "            'them',\n",
       "            'themselves',\n",
       "            'then',\n",
       "            'thence',\n",
       "            'there',\n",
       "            'thereafter',\n",
       "            'thereby',\n",
       "            'therefore',\n",
       "            'therein',\n",
       "            'thereupon',\n",
       "            'these',\n",
       "            'they',\n",
       "            'thick',\n",
       "            'thin',\n",
       "            'third',\n",
       "            'this',\n",
       "            'those',\n",
       "            'though',\n",
       "            'three',\n",
       "            'through',\n",
       "            'throughout',\n",
       "            'thru',\n",
       "            'thus',\n",
       "            'to',\n",
       "            'together',\n",
       "            'too',\n",
       "            'top',\n",
       "            'toward',\n",
       "            'towards',\n",
       "            'twelve',\n",
       "            'twenty',\n",
       "            'two',\n",
       "            'un',\n",
       "            'under',\n",
       "            'until',\n",
       "            'up',\n",
       "            'upon',\n",
       "            'us',\n",
       "            've',\n",
       "            'very',\n",
       "            'via',\n",
       "            'wa',\n",
       "            'was',\n",
       "            'we',\n",
       "            'well',\n",
       "            'were',\n",
       "            'what',\n",
       "            'whatever',\n",
       "            'when',\n",
       "            'whence',\n",
       "            'whenever',\n",
       "            'where',\n",
       "            'whereafter',\n",
       "            'whereas',\n",
       "            'whereby',\n",
       "            'wherein',\n",
       "            'whereupon',\n",
       "            'wherever',\n",
       "            'whether',\n",
       "            'which',\n",
       "            'while',\n",
       "            'whither',\n",
       "            'who',\n",
       "            'whoever',\n",
       "            'whole',\n",
       "            'whom',\n",
       "            'whose',\n",
       "            'why',\n",
       "            'will',\n",
       "            'with',\n",
       "            'within',\n",
       "            'without',\n",
       "            'would',\n",
       "            'yet',\n",
       "            'you',\n",
       "            'your',\n",
       "            'yours',\n",
       "            'yourself',\n",
       "            'yourselves'}),\n",
       " 'knn__n_neighbors': 5,\n",
       " 'knn__p': 2,\n",
       " 'knn__weights': 'uniform'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cvec_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8293259410563175"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cvec_knn.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8755020080321285"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training score\n",
    "gs_cvec_knn.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112449799196787"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test score\n",
    "gs_cvec_knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_cvec__max_df</th>\n",
       "      <th>param_cvec__max_features</th>\n",
       "      <th>param_cvec__min_df</th>\n",
       "      <th>param_cvec__ngram_range</th>\n",
       "      <th>param_knn__n_neighbors</th>\n",
       "      <th>param_knn__p</th>\n",
       "      <th>param_knn__weights</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1200</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.829326</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1200</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.829326</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1200</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.829326</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1200</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.829326</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1200</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.825326</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.95</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.598413</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.572286</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.95</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.572286</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.571617</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.95</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.571617</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_cvec__max_df param_cvec__max_features param_cvec__min_df  \\\n",
       "62               0.95                     1200                  2   \n",
       "15                0.9                     1200                  2   \n",
       "14                0.9                     1200                  2   \n",
       "63               0.95                     1200                  2   \n",
       "51               0.95                     1200                  2   \n",
       "..                ...                      ...                ...   \n",
       "92               0.95                     2000                  2   \n",
       "41                0.9                     2000                  2   \n",
       "89               0.95                     2000                  2   \n",
       "40                0.9                     2000                  2   \n",
       "88               0.95                     2000                  2   \n",
       "\n",
       "   param_cvec__ngram_range param_knn__n_neighbors param_knn__p  \\\n",
       "62                  (1, 2)                      5            2   \n",
       "15                  (1, 2)                      5            2   \n",
       "14                  (1, 2)                      5            2   \n",
       "63                  (1, 2)                      5            2   \n",
       "51                  (1, 1)                      3            2   \n",
       "..                     ...                    ...          ...   \n",
       "92                  (1, 2)                      5            1   \n",
       "41                  (1, 2)                      3            1   \n",
       "89                  (1, 2)                      3            1   \n",
       "40                  (1, 2)                      3            1   \n",
       "88                  (1, 2)                      3            1   \n",
       "\n",
       "   param_knn__weights  mean_test_score  rank_test_score  \n",
       "62            uniform         0.829326                1  \n",
       "15           distance         0.829326                1  \n",
       "14            uniform         0.829326                1  \n",
       "63           distance         0.829326                1  \n",
       "51           distance         0.825326                5  \n",
       "..                ...              ...              ...  \n",
       "92            uniform         0.598413               91  \n",
       "41           distance         0.572286               93  \n",
       "89           distance         0.572286               93  \n",
       "40            uniform         0.571617               95  \n",
       "88            uniform         0.571617               95  \n",
       "\n",
       "[96 rows x 9 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a dataframe to show the best rank score from the different params\n",
    "score3_df = pd.DataFrame(gs_cvec_knn.cv_results_).sort_values('rank_test_score')\n",
    "\n",
    "score3_df = score3_df[['param_cvec__max_df', 'param_cvec__max_features', 'param_cvec__min_df',\n",
    "       'param_cvec__ngram_range', 'param_knn__n_neighbors', 'param_knn__p','param_knn__weights', 'mean_test_score','rank_test_score']]\n",
    "score3_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hese are the top 5 results of GridSearchCV with customized cvec parameters and KNearestNeighbors. The cvec params that were customized are the same, \n",
    "\n",
    "- param_knn_n_neighbors: Number of neighbors to use. With reference to the first ranking model, 5 neighbors were used in that model.\n",
    "\n",
    "- param_knn_p: The type of metric used where 2 represents euclidean_distance.\n",
    "\n",
    "- param_knn_weights: Uniform representing all points in each neighborhood are weighted equally whereas Distance weight points by the inverse of their distance, closer neighbors of a query point will have a greater influence than neighbors which are further away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 241\n",
      "False Positives: 7\n",
      "False Negatives: 87\n",
      "True Positives: 163\n",
      "\n",
      "Accuracy: 0.8112449799196787\n",
      "Misclassification: 0.1887550200803213\n",
      "Precision: 0.9588235294117647\n",
      "Sensitivity: 0.652\n",
      "Specificity: 0.9717741935483871\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_matrix(X_test,gs_cvec_knn,'Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- Accuracy: 0.8112\n",
    "- True Negative: 241\n",
    "- False Negative: 87\n",
    "- Specificity: 0.9717\n",
    "- Sensitivity: 0.652\n",
    "\n",
    "The training and test accuracy scores for this model are 87.55% and 81.12% respectively suggesting that the model is overfitted. The false negative count have increase to 87 contributing to the drop in sensitivity score to 65.2%. This is a fairly bad model as other than the false positive count moving in the right direction, all other counts were going towards the opposite direction. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using tf-idf vectorizer with KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pipeline with tf-idf vectorizer as the transformer and K-NearestNeighbors as the estimator\n",
    "pipe_tfid_knn = Pipeline([\n",
    "    ('tfid',TfidfVectorizer()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params_tfid_knn = {\n",
    "    'tfid__max_features': [1200,1600,2000],\n",
    "    'tfid__min_df': [2],\n",
    "    'tfid__max_df': [.9, .95],\n",
    "    'tfid__stop_words': [stop_words],\n",
    "    'tfid__ngram_range': [(1,1), (1,2)],\n",
    "    'knn__p': [1, 2], \n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__n_neighbors': [3, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tfid_knn = GridSearchCV(pipe_tfid_knn, # Pipeline that we created\n",
    "                         param_grid = pipe_params_tfid_knn, # Using the customized params that we want to search over\n",
    "                         cv = 5, # 5-fold cross validation\n",
    "                         verbose=1,\n",
    "                           n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   36.6s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tfid', TfidfVectorizer()),\n",
       "                                       ('knn', KNeighborsClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'knn__n_neighbors': [3, 5], 'knn__p': [1, 2],\n",
       "                         'knn__weights': ['uniform', 'distance'],\n",
       "                         'tfid__max_df': [0.9, 0.95],\n",
       "                         'tfid__max_features': [1200, 1600, 2000],\n",
       "                         'tfid__min_df': [2],\n",
       "                         'tfid__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'tfid__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                         'across', 'after',\n",
       "                                                         'afterwards', 'again',\n",
       "                                                         'against', 'all',\n",
       "                                                         'almost', 'alone',\n",
       "                                                         'along', 'already',\n",
       "                                                         'also', 'although',\n",
       "                                                         'always', 'am',\n",
       "                                                         'among', 'amongst',\n",
       "                                                         'amoungst', 'amount',\n",
       "                                                         'amp', 'an', 'and',\n",
       "                                                         'another', 'any',\n",
       "                                                         'anyhow', 'anyone',\n",
       "                                                         'anything', 'anyway', ...})]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tfid_knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'knn__n_neighbors': 5,\n",
       " 'knn__p': 2,\n",
       " 'knn__weights': 'distance',\n",
       " 'tfid__max_df': 0.9,\n",
       " 'tfid__max_features': 2000,\n",
       " 'tfid__min_df': 2,\n",
       " 'tfid__ngram_range': (1, 2),\n",
       " 'tfid__stop_words': frozenset({'a',\n",
       "            'about',\n",
       "            'above',\n",
       "            'across',\n",
       "            'after',\n",
       "            'afterwards',\n",
       "            'again',\n",
       "            'against',\n",
       "            'all',\n",
       "            'almost',\n",
       "            'alone',\n",
       "            'along',\n",
       "            'already',\n",
       "            'also',\n",
       "            'although',\n",
       "            'always',\n",
       "            'am',\n",
       "            'among',\n",
       "            'amongst',\n",
       "            'amoungst',\n",
       "            'amount',\n",
       "            'amp',\n",
       "            'an',\n",
       "            'and',\n",
       "            'another',\n",
       "            'any',\n",
       "            'anyhow',\n",
       "            'anyone',\n",
       "            'anything',\n",
       "            'anyway',\n",
       "            'anywhere',\n",
       "            'are',\n",
       "            'around',\n",
       "            'as',\n",
       "            'at',\n",
       "            'back',\n",
       "            'be',\n",
       "            'became',\n",
       "            'because',\n",
       "            'become',\n",
       "            'becomes',\n",
       "            'becoming',\n",
       "            'been',\n",
       "            'before',\n",
       "            'beforehand',\n",
       "            'behind',\n",
       "            'being',\n",
       "            'below',\n",
       "            'beside',\n",
       "            'besides',\n",
       "            'between',\n",
       "            'beyond',\n",
       "            'bill',\n",
       "            'both',\n",
       "            'bottom',\n",
       "            'but',\n",
       "            'by',\n",
       "            'call',\n",
       "            'can',\n",
       "            'cannot',\n",
       "            'cant',\n",
       "            'co',\n",
       "            'con',\n",
       "            'could',\n",
       "            'couldnt',\n",
       "            'cry',\n",
       "            'de',\n",
       "            'describe',\n",
       "            'detail',\n",
       "            'do',\n",
       "            'doesn',\n",
       "            'don',\n",
       "            'done',\n",
       "            'down',\n",
       "            'due',\n",
       "            'during',\n",
       "            'each',\n",
       "            'eg',\n",
       "            'eight',\n",
       "            'either',\n",
       "            'eleven',\n",
       "            'else',\n",
       "            'elsewhere',\n",
       "            'empty',\n",
       "            'enough',\n",
       "            'etc',\n",
       "            'even',\n",
       "            'ever',\n",
       "            'every',\n",
       "            'everyone',\n",
       "            'everything',\n",
       "            'everywhere',\n",
       "            'except',\n",
       "            'few',\n",
       "            'fifteen',\n",
       "            'fifty',\n",
       "            'fill',\n",
       "            'find',\n",
       "            'fire',\n",
       "            'first',\n",
       "            'five',\n",
       "            'for',\n",
       "            'former',\n",
       "            'formerly',\n",
       "            'forty',\n",
       "            'found',\n",
       "            'four',\n",
       "            'from',\n",
       "            'front',\n",
       "            'full',\n",
       "            'further',\n",
       "            'get',\n",
       "            'getting',\n",
       "            'give',\n",
       "            'go',\n",
       "            'going',\n",
       "            'got',\n",
       "            'ha',\n",
       "            'had',\n",
       "            'has',\n",
       "            'hasnt',\n",
       "            'have',\n",
       "            'he',\n",
       "            'hence',\n",
       "            'her',\n",
       "            'here',\n",
       "            'hereafter',\n",
       "            'hereby',\n",
       "            'herein',\n",
       "            'hereupon',\n",
       "            'hers',\n",
       "            'herself',\n",
       "            'him',\n",
       "            'himself',\n",
       "            'his',\n",
       "            'how',\n",
       "            'however',\n",
       "            'https',\n",
       "            'hundred',\n",
       "            'i',\n",
       "            'i m',\n",
       "            'ie',\n",
       "            'if',\n",
       "            'in',\n",
       "            'inc',\n",
       "            'indeed',\n",
       "            'interest',\n",
       "            'into',\n",
       "            'is',\n",
       "            'isn',\n",
       "            'it',\n",
       "            'its',\n",
       "            'itself',\n",
       "            'keep',\n",
       "            'last',\n",
       "            'latter',\n",
       "            'latterly',\n",
       "            'least',\n",
       "            'less',\n",
       "            'ltd',\n",
       "            'made',\n",
       "            'many',\n",
       "            'may',\n",
       "            'me',\n",
       "            'meanwhile',\n",
       "            'might',\n",
       "            'mill',\n",
       "            'mine',\n",
       "            'more',\n",
       "            'moreover',\n",
       "            'most',\n",
       "            'mostly',\n",
       "            'move',\n",
       "            'much',\n",
       "            'must',\n",
       "            'my',\n",
       "            'myself',\n",
       "            'name',\n",
       "            'namely',\n",
       "            'neither',\n",
       "            'never',\n",
       "            'nevertheless',\n",
       "            'next',\n",
       "            'nine',\n",
       "            'no',\n",
       "            'nobody',\n",
       "            'none',\n",
       "            'noone',\n",
       "            'nor',\n",
       "            'not',\n",
       "            'nothing',\n",
       "            'now',\n",
       "            'nowhere',\n",
       "            'of',\n",
       "            'off',\n",
       "            'often',\n",
       "            'on',\n",
       "            'once',\n",
       "            'one',\n",
       "            'only',\n",
       "            'onto',\n",
       "            'or',\n",
       "            'other',\n",
       "            'others',\n",
       "            'otherwise',\n",
       "            'our',\n",
       "            'ours',\n",
       "            'ourselves',\n",
       "            'out',\n",
       "            'over',\n",
       "            'own',\n",
       "            'part',\n",
       "            'per',\n",
       "            'perhaps',\n",
       "            'please',\n",
       "            'put',\n",
       "            'rather',\n",
       "            're',\n",
       "            'same',\n",
       "            'see',\n",
       "            'seem',\n",
       "            'seemed',\n",
       "            'seeming',\n",
       "            'seems',\n",
       "            'serious',\n",
       "            'several',\n",
       "            'she',\n",
       "            'should',\n",
       "            'show',\n",
       "            'side',\n",
       "            'since',\n",
       "            'sincere',\n",
       "            'six',\n",
       "            'sixty',\n",
       "            'so',\n",
       "            'some',\n",
       "            'somehow',\n",
       "            'someone',\n",
       "            'something',\n",
       "            'sometime',\n",
       "            'sometimes',\n",
       "            'somewhere',\n",
       "            'still',\n",
       "            'such',\n",
       "            'system',\n",
       "            'take',\n",
       "            'ten',\n",
       "            'than',\n",
       "            'that',\n",
       "            'the',\n",
       "            'their',\n",
       "            'them',\n",
       "            'themselves',\n",
       "            'then',\n",
       "            'thence',\n",
       "            'there',\n",
       "            'thereafter',\n",
       "            'thereby',\n",
       "            'therefore',\n",
       "            'therein',\n",
       "            'thereupon',\n",
       "            'these',\n",
       "            'they',\n",
       "            'thick',\n",
       "            'thin',\n",
       "            'third',\n",
       "            'this',\n",
       "            'those',\n",
       "            'though',\n",
       "            'three',\n",
       "            'through',\n",
       "            'throughout',\n",
       "            'thru',\n",
       "            'thus',\n",
       "            'to',\n",
       "            'together',\n",
       "            'too',\n",
       "            'top',\n",
       "            'toward',\n",
       "            'towards',\n",
       "            'twelve',\n",
       "            'twenty',\n",
       "            'two',\n",
       "            'un',\n",
       "            'under',\n",
       "            'until',\n",
       "            'up',\n",
       "            'upon',\n",
       "            'us',\n",
       "            've',\n",
       "            'very',\n",
       "            'via',\n",
       "            'wa',\n",
       "            'was',\n",
       "            'we',\n",
       "            'well',\n",
       "            'were',\n",
       "            'what',\n",
       "            'whatever',\n",
       "            'when',\n",
       "            'whence',\n",
       "            'whenever',\n",
       "            'where',\n",
       "            'whereafter',\n",
       "            'whereas',\n",
       "            'whereby',\n",
       "            'wherein',\n",
       "            'whereupon',\n",
       "            'wherever',\n",
       "            'whether',\n",
       "            'which',\n",
       "            'while',\n",
       "            'whither',\n",
       "            'who',\n",
       "            'whoever',\n",
       "            'whole',\n",
       "            'whom',\n",
       "            'whose',\n",
       "            'why',\n",
       "            'will',\n",
       "            'with',\n",
       "            'within',\n",
       "            'without',\n",
       "            'would',\n",
       "            'yet',\n",
       "            'you',\n",
       "            'your',\n",
       "            'yours',\n",
       "            'yourself',\n",
       "            'yourselves'})}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tfid_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9196830598639762"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tfid_knn.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training score\n",
    "gs_tfid_knn.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9236947791164659"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test score\n",
    "gs_tfid_knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_tfid__max_df</th>\n",
       "      <th>param_tfid__max_features</th>\n",
       "      <th>param_tfid__min_df</th>\n",
       "      <th>param_tfid__ngram_range</th>\n",
       "      <th>param_knn__n_neighbors</th>\n",
       "      <th>param_knn__p</th>\n",
       "      <th>param_knn__weights</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.95</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.919683</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.919683</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.95</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.919014</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.919014</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1600</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.918350</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1600</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.567612</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1600</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.566275</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1600</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.566275</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.95</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.565586</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.565586</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_tfid__max_df param_tfid__max_features param_tfid__min_df  \\\n",
       "95               0.95                     2000                  2   \n",
       "89                0.9                     2000                  2   \n",
       "83               0.95                     2000                  2   \n",
       "77                0.9                     2000                  2   \n",
       "93               0.95                     1600                  2   \n",
       "..                ...                      ...                ...   \n",
       "15                0.9                     1600                  2   \n",
       "3                 0.9                     1600                  2   \n",
       "9                0.95                     1600                  2   \n",
       "58               0.95                     2000                  2   \n",
       "52                0.9                     2000                  2   \n",
       "\n",
       "   param_tfid__ngram_range param_knn__n_neighbors param_knn__p  \\\n",
       "95                  (1, 2)                      5            2   \n",
       "89                  (1, 2)                      5            2   \n",
       "83                  (1, 2)                      5            2   \n",
       "77                  (1, 2)                      5            2   \n",
       "93                  (1, 2)                      5            2   \n",
       "..                     ...                    ...          ...   \n",
       "15                  (1, 2)                      3            1   \n",
       "3                   (1, 2)                      3            1   \n",
       "9                   (1, 2)                      3            1   \n",
       "58                  (1, 1)                      5            1   \n",
       "52                  (1, 1)                      5            1   \n",
       "\n",
       "   param_knn__weights  mean_test_score  rank_test_score  \n",
       "95           distance         0.919683                1  \n",
       "89           distance         0.919683                1  \n",
       "83            uniform         0.919014                3  \n",
       "77            uniform         0.919014                3  \n",
       "93           distance         0.918350                5  \n",
       "..                ...              ...              ...  \n",
       "15           distance         0.567612               91  \n",
       "3             uniform         0.566275               93  \n",
       "9             uniform         0.566275               93  \n",
       "58            uniform         0.565586               95  \n",
       "52            uniform         0.565586               95  \n",
       "\n",
       "[96 rows x 9 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a dataframe to show the best rank score from the different params\n",
    "score4_df = pd.DataFrame(gs_tfid_knn.cv_results_).sort_values('rank_test_score')\n",
    "\n",
    "score4_df = score4_df[['param_tfid__max_df', 'param_tfid__max_features', 'param_tfid__min_df',\n",
    "       'param_tfid__ngram_range', 'param_knn__n_neighbors', 'param_knn__p','param_knn__weights', 'mean_test_score','rank_test_score']]\n",
    "score4_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 227\n",
      "False Positives: 21\n",
      "False Negatives: 17\n",
      "True Positives: 233\n",
      "\n",
      "Accuracy: 0.9236947791164659\n",
      "Misclassification: 0.07630522088353409\n",
      "Precision: 0.9173228346456693\n",
      "Sensitivity: 0.932\n",
      "Specificity: 0.9153225806451613\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_matrix(X_test,gs_tfid_knn,'Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- Accuracy: 0.9236\n",
    "- True Negative: 227\n",
    "- False Negative: 17\n",
    "- Specificity: 0.9153\n",
    "- Sensitivity: 0.932\n",
    "\n",
    "The training and test accuracy scores for this model are 100%, 92.36% respectively indicating that the model is overfitted.\n",
    "The false negative count has dropped to 17 as compared to the previous model, contributing to the rise of sensitivity score to 93.2%. Specificity score is at 91.53% indicating the accuracy of predicted vegan posts that were correctly identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Countvectorizer with RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pipeline with countvectorizer as the transformer and Random Forest as the estimator\n",
    "pipe_cvec_rf = Pipeline([\n",
    "    ('cvec',CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params_cvec_rf = {\n",
    "    'cvec__max_features': [1200,1600,2000],\n",
    "    'cvec__min_df': [2],\n",
    "    'cvec__max_df': [.9],\n",
    "    'cvec__stop_words': [stop_words],\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'rf__n_estimators': [100,150,200],\n",
    "    'rf__max_depth': [1,2,3,4,5],\n",
    "    'rf__max_features':['auto','log2',1/3],\n",
    "    'rf__random_state': [42]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cvec_rf = GridSearchCV(pipe_cvec_rf, # Pipeline that we created\n",
    "                          param_grid= pipe_params_cvec_rf, # Using the customized params that we want to search over\n",
    "                          cv=5, # 5-fold cross validation\n",
    "                          verbose=1,\n",
    "                          n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 135 candidates, totalling 675 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 675 out of 675 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('rf', RandomForestClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'cvec__max_df': [0.9],\n",
       "                         'cvec__max_features': [1200, 1600, 2000],\n",
       "                         'cvec__min_df': [2], 'cvec__ngram_range': [(1, 2)],\n",
       "                         'cvec__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                         'across', 'after',\n",
       "                                                         'afterwards', 'again',\n",
       "                                                         'against', 'all',\n",
       "                                                         'almost', 'alone',\n",
       "                                                         'along', 'already',\n",
       "                                                         'also', 'although',\n",
       "                                                         'always', 'am',\n",
       "                                                         'among', 'amongst',\n",
       "                                                         'amoungst', 'amount',\n",
       "                                                         'amp', 'an', 'and',\n",
       "                                                         'another', 'any',\n",
       "                                                         'anyhow', 'anyone',\n",
       "                                                         'anything', 'anyway', ...})],\n",
       "                         'rf__max_depth': [1, 2, 3, 4, 5],\n",
       "                         'rf__max_features': ['auto', 'log2',\n",
       "                                              0.3333333333333333],\n",
       "                         'rf__n_estimators': [100, 150, 200],\n",
       "                         'rf__random_state': [42]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cvec_rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__max_features': 1600,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'cvec__stop_words': frozenset({'a',\n",
       "            'about',\n",
       "            'above',\n",
       "            'across',\n",
       "            'after',\n",
       "            'afterwards',\n",
       "            'again',\n",
       "            'against',\n",
       "            'all',\n",
       "            'almost',\n",
       "            'alone',\n",
       "            'along',\n",
       "            'already',\n",
       "            'also',\n",
       "            'although',\n",
       "            'always',\n",
       "            'am',\n",
       "            'among',\n",
       "            'amongst',\n",
       "            'amoungst',\n",
       "            'amount',\n",
       "            'amp',\n",
       "            'an',\n",
       "            'and',\n",
       "            'another',\n",
       "            'any',\n",
       "            'anyhow',\n",
       "            'anyone',\n",
       "            'anything',\n",
       "            'anyway',\n",
       "            'anywhere',\n",
       "            'are',\n",
       "            'around',\n",
       "            'as',\n",
       "            'at',\n",
       "            'back',\n",
       "            'be',\n",
       "            'became',\n",
       "            'because',\n",
       "            'become',\n",
       "            'becomes',\n",
       "            'becoming',\n",
       "            'been',\n",
       "            'before',\n",
       "            'beforehand',\n",
       "            'behind',\n",
       "            'being',\n",
       "            'below',\n",
       "            'beside',\n",
       "            'besides',\n",
       "            'between',\n",
       "            'beyond',\n",
       "            'bill',\n",
       "            'both',\n",
       "            'bottom',\n",
       "            'but',\n",
       "            'by',\n",
       "            'call',\n",
       "            'can',\n",
       "            'cannot',\n",
       "            'cant',\n",
       "            'co',\n",
       "            'con',\n",
       "            'could',\n",
       "            'couldnt',\n",
       "            'cry',\n",
       "            'de',\n",
       "            'describe',\n",
       "            'detail',\n",
       "            'do',\n",
       "            'doesn',\n",
       "            'don',\n",
       "            'done',\n",
       "            'down',\n",
       "            'due',\n",
       "            'during',\n",
       "            'each',\n",
       "            'eg',\n",
       "            'eight',\n",
       "            'either',\n",
       "            'eleven',\n",
       "            'else',\n",
       "            'elsewhere',\n",
       "            'empty',\n",
       "            'enough',\n",
       "            'etc',\n",
       "            'even',\n",
       "            'ever',\n",
       "            'every',\n",
       "            'everyone',\n",
       "            'everything',\n",
       "            'everywhere',\n",
       "            'except',\n",
       "            'few',\n",
       "            'fifteen',\n",
       "            'fifty',\n",
       "            'fill',\n",
       "            'find',\n",
       "            'fire',\n",
       "            'first',\n",
       "            'five',\n",
       "            'for',\n",
       "            'former',\n",
       "            'formerly',\n",
       "            'forty',\n",
       "            'found',\n",
       "            'four',\n",
       "            'from',\n",
       "            'front',\n",
       "            'full',\n",
       "            'further',\n",
       "            'get',\n",
       "            'getting',\n",
       "            'give',\n",
       "            'go',\n",
       "            'going',\n",
       "            'got',\n",
       "            'ha',\n",
       "            'had',\n",
       "            'has',\n",
       "            'hasnt',\n",
       "            'have',\n",
       "            'he',\n",
       "            'hence',\n",
       "            'her',\n",
       "            'here',\n",
       "            'hereafter',\n",
       "            'hereby',\n",
       "            'herein',\n",
       "            'hereupon',\n",
       "            'hers',\n",
       "            'herself',\n",
       "            'him',\n",
       "            'himself',\n",
       "            'his',\n",
       "            'how',\n",
       "            'however',\n",
       "            'https',\n",
       "            'hundred',\n",
       "            'i',\n",
       "            'i m',\n",
       "            'ie',\n",
       "            'if',\n",
       "            'in',\n",
       "            'inc',\n",
       "            'indeed',\n",
       "            'interest',\n",
       "            'into',\n",
       "            'is',\n",
       "            'isn',\n",
       "            'it',\n",
       "            'its',\n",
       "            'itself',\n",
       "            'keep',\n",
       "            'last',\n",
       "            'latter',\n",
       "            'latterly',\n",
       "            'least',\n",
       "            'less',\n",
       "            'ltd',\n",
       "            'made',\n",
       "            'many',\n",
       "            'may',\n",
       "            'me',\n",
       "            'meanwhile',\n",
       "            'might',\n",
       "            'mill',\n",
       "            'mine',\n",
       "            'more',\n",
       "            'moreover',\n",
       "            'most',\n",
       "            'mostly',\n",
       "            'move',\n",
       "            'much',\n",
       "            'must',\n",
       "            'my',\n",
       "            'myself',\n",
       "            'name',\n",
       "            'namely',\n",
       "            'neither',\n",
       "            'never',\n",
       "            'nevertheless',\n",
       "            'next',\n",
       "            'nine',\n",
       "            'no',\n",
       "            'nobody',\n",
       "            'none',\n",
       "            'noone',\n",
       "            'nor',\n",
       "            'not',\n",
       "            'nothing',\n",
       "            'now',\n",
       "            'nowhere',\n",
       "            'of',\n",
       "            'off',\n",
       "            'often',\n",
       "            'on',\n",
       "            'once',\n",
       "            'one',\n",
       "            'only',\n",
       "            'onto',\n",
       "            'or',\n",
       "            'other',\n",
       "            'others',\n",
       "            'otherwise',\n",
       "            'our',\n",
       "            'ours',\n",
       "            'ourselves',\n",
       "            'out',\n",
       "            'over',\n",
       "            'own',\n",
       "            'part',\n",
       "            'per',\n",
       "            'perhaps',\n",
       "            'please',\n",
       "            'put',\n",
       "            'rather',\n",
       "            're',\n",
       "            'same',\n",
       "            'see',\n",
       "            'seem',\n",
       "            'seemed',\n",
       "            'seeming',\n",
       "            'seems',\n",
       "            'serious',\n",
       "            'several',\n",
       "            'she',\n",
       "            'should',\n",
       "            'show',\n",
       "            'side',\n",
       "            'since',\n",
       "            'sincere',\n",
       "            'six',\n",
       "            'sixty',\n",
       "            'so',\n",
       "            'some',\n",
       "            'somehow',\n",
       "            'someone',\n",
       "            'something',\n",
       "            'sometime',\n",
       "            'sometimes',\n",
       "            'somewhere',\n",
       "            'still',\n",
       "            'such',\n",
       "            'system',\n",
       "            'take',\n",
       "            'ten',\n",
       "            'than',\n",
       "            'that',\n",
       "            'the',\n",
       "            'their',\n",
       "            'them',\n",
       "            'themselves',\n",
       "            'then',\n",
       "            'thence',\n",
       "            'there',\n",
       "            'thereafter',\n",
       "            'thereby',\n",
       "            'therefore',\n",
       "            'therein',\n",
       "            'thereupon',\n",
       "            'these',\n",
       "            'they',\n",
       "            'thick',\n",
       "            'thin',\n",
       "            'third',\n",
       "            'this',\n",
       "            'those',\n",
       "            'though',\n",
       "            'three',\n",
       "            'through',\n",
       "            'throughout',\n",
       "            'thru',\n",
       "            'thus',\n",
       "            'to',\n",
       "            'together',\n",
       "            'too',\n",
       "            'top',\n",
       "            'toward',\n",
       "            'towards',\n",
       "            'twelve',\n",
       "            'twenty',\n",
       "            'two',\n",
       "            'un',\n",
       "            'under',\n",
       "            'until',\n",
       "            'up',\n",
       "            'upon',\n",
       "            'us',\n",
       "            've',\n",
       "            'very',\n",
       "            'via',\n",
       "            'wa',\n",
       "            'was',\n",
       "            'we',\n",
       "            'well',\n",
       "            'were',\n",
       "            'what',\n",
       "            'whatever',\n",
       "            'when',\n",
       "            'whence',\n",
       "            'whenever',\n",
       "            'where',\n",
       "            'whereafter',\n",
       "            'whereas',\n",
       "            'whereby',\n",
       "            'wherein',\n",
       "            'whereupon',\n",
       "            'wherever',\n",
       "            'whether',\n",
       "            'which',\n",
       "            'while',\n",
       "            'whither',\n",
       "            'who',\n",
       "            'whoever',\n",
       "            'whole',\n",
       "            'whom',\n",
       "            'whose',\n",
       "            'why',\n",
       "            'will',\n",
       "            'with',\n",
       "            'within',\n",
       "            'without',\n",
       "            'would',\n",
       "            'yet',\n",
       "            'you',\n",
       "            'your',\n",
       "            'yours',\n",
       "            'yourself',\n",
       "            'yourselves'}),\n",
       " 'rf__max_depth': 1,\n",
       " 'rf__max_features': 'auto',\n",
       " 'rf__n_estimators': 200,\n",
       " 'rf__random_state': 42}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cvec_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9578438194428858"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cvec_rf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.963855421686747"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training score\n",
    "gs_cvec_rf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9658634538152611"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test score\n",
    "gs_cvec_rf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_cvec__max_df</th>\n",
       "      <th>param_cvec__max_features</th>\n",
       "      <th>param_cvec__min_df</th>\n",
       "      <th>param_cvec__ngram_range</th>\n",
       "      <th>param_rf__max_depth</th>\n",
       "      <th>param_rf__max_features</th>\n",
       "      <th>param_rf__n_estimators</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1600</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>0.957844</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1200</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>150</td>\n",
       "      <td>0.957842</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1600</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>2</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>0.957175</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>auto</td>\n",
       "      <td>150</td>\n",
       "      <td>0.957173</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1200</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.957168</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1600</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.910339</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.898925</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.896249</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1</td>\n",
       "      <td>log2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.878847</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.834664</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    param_cvec__max_df param_cvec__max_features param_cvec__min_df  \\\n",
       "47                 0.9                     1600                  2   \n",
       "37                 0.9                     1200                  2   \n",
       "56                 0.9                     1600                  2   \n",
       "109                0.9                     2000                  2   \n",
       "40                 0.9                     1200                  2   \n",
       "..                 ...                      ...                ...   \n",
       "48                 0.9                     1600                  2   \n",
       "95                 0.9                     2000                  2   \n",
       "102                0.9                     2000                  2   \n",
       "94                 0.9                     2000                  2   \n",
       "93                 0.9                     2000                  2   \n",
       "\n",
       "    param_cvec__ngram_range param_rf__max_depth param_rf__max_features  \\\n",
       "47                   (1, 2)                   1                   auto   \n",
       "37                   (1, 2)                   5                   auto   \n",
       "56                   (1, 2)                   2                   auto   \n",
       "109                  (1, 2)                   3                   auto   \n",
       "40                   (1, 2)                   5                   log2   \n",
       "..                      ...                 ...                    ...   \n",
       "48                   (1, 2)                   1                   log2   \n",
       "95                   (1, 2)                   1                   log2   \n",
       "102                  (1, 2)                   2                   log2   \n",
       "94                   (1, 2)                   1                   log2   \n",
       "93                   (1, 2)                   1                   log2   \n",
       "\n",
       "    param_rf__n_estimators  mean_test_score  rank_test_score  \n",
       "47                     200         0.957844                1  \n",
       "37                     150         0.957842                2  \n",
       "56                     200         0.957175                3  \n",
       "109                    150         0.957173                4  \n",
       "40                     150         0.957168                5  \n",
       "..                     ...              ...              ...  \n",
       "48                     100         0.910339              131  \n",
       "95                     200         0.898925              132  \n",
       "102                    100         0.896249              133  \n",
       "94                     150         0.878847              134  \n",
       "93                     100         0.834664              135  \n",
       "\n",
       "[135 rows x 9 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a dataframe to show the best rank score from the different params\n",
    "score5_df = pd.DataFrame(gs_cvec_rf.cv_results_).sort_values('rank_test_score')\n",
    "\n",
    "score5_df = score5_df[['param_cvec__max_df', 'param_cvec__max_features', 'param_cvec__min_df',\n",
    "       'param_cvec__ngram_range','param_rf__max_depth', 'param_rf__max_features',\n",
    "       'param_rf__n_estimators' ,'mean_test_score','rank_test_score']]\n",
    "score5_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the top 5 results of GridSearchCV with customized cvec parameters and Random Forest. Using the same customized cvec parameters, let us go through the random forest parameters that were customized:\n",
    "\n",
    "- param_rf_max_depth: The maximum depth of the tree. Using the first ranking model, this means that the model has a depth of 4.\n",
    "\n",
    "- param_rf_max_features: The number of features to consider when looking for the best split:\n",
    "  - If float, then `max_features` is a fraction and `int(max_features * n_features)` features are considered at each split.\n",
    "  - If \"auto\", then `max_features=sqrt(n_features)`.\n",
    "  - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
    "  - If \"log2\", then `max_features=log2(n_features)`.\n",
    "\n",
    "  The first ranking model uses auto which means its the square root of the number of features.\n",
    "\n",
    "\n",
    "- param_rf_n_estimators: The number of trees in the forest. Referencing from the first ranking model, there are 200 trees in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 238\n",
      "False Positives: 10\n",
      "False Negatives: 7\n",
      "True Positives: 243\n",
      "\n",
      "Accuracy: 0.9658634538152611\n",
      "Misclassification: 0.034136546184738936\n",
      "Precision: 0.9604743083003953\n",
      "Sensitivity: 0.972\n",
      "Specificity: 0.9596774193548387\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_matrix(X_test,gs_cvec_rf,'Greens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- Accuracy: 0.9658\n",
    "- True Negative: 238\n",
    "- False Negative: 7\n",
    "- Specificity: 0.9596\n",
    "- Sensitivity: 0.972\n",
    "\n",
    "The training and test accuracy scores for this model are 96.38% and 96.58% respectively indicating that the model is well fitted as the scores are very similar. The false negative count have improved to 7 which probably carried the sensitivity score to 97.20% which is the highest compared to the other previous models. The predicted vegan post that were correctly identified(True negative) is at 238 and specificity score 95.96%. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using tf-idf vectorizer with RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pipeline with tf-idf vectorizer as the transformer and Random Forest as the estimator\n",
    "pipe_tfid_rf = Pipeline([\n",
    "    ('tfid',TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params_tfid_rf = {\n",
    "    'tfid__max_features': [1200,1600,2000],\n",
    "    'tfid__min_df': [2],\n",
    "    'tfid__max_df': [.9],\n",
    "    'tfid__stop_words': [stop_words],\n",
    "    'tfid__ngram_range': [(1,2)],\n",
    "    'rf__n_estimators': [100,150,200],\n",
    "    'rf__max_depth': [1,2,3,4,5],\n",
    "    'rf__max_features':['auto','log2',1/3],\n",
    "    'rf__random_state':[42]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tfid_rf = GridSearchCV(pipe_tfid_rf, # Pipeline that we created\n",
    "                          param_grid= pipe_params_tfid_rf, #Using the customized params that we want to search over\n",
    "                          cv=5, # 5-fold cross-validation\n",
    "                          verbose=1,\n",
    "                          n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 135 candidates, totalling 675 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 675 out of 675 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tfid', TfidfVectorizer()),\n",
       "                                       ('rf', RandomForestClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'rf__max_depth': [1, 2, 3, 4, 5],\n",
       "                         'rf__max_features': ['auto', 'log2',\n",
       "                                              0.3333333333333333],\n",
       "                         'rf__n_estimators': [100, 150, 200],\n",
       "                         'rf__random_state': [42], 'tfid__max_df': [0.9],\n",
       "                         'tfid__max_features': [1200, 1600, 2000],\n",
       "                         'tfid__min_df': [2], 'tfid__ngram_range': [(1, 2)],\n",
       "                         'tfid__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                         'across', 'after',\n",
       "                                                         'afterwards', 'again',\n",
       "                                                         'against', 'all',\n",
       "                                                         'almost', 'alone',\n",
       "                                                         'along', 'already',\n",
       "                                                         'also', 'although',\n",
       "                                                         'always', 'am',\n",
       "                                                         'among', 'amongst',\n",
       "                                                         'amoungst', 'amount',\n",
       "                                                         'amp', 'an', 'and',\n",
       "                                                         'another', 'any',\n",
       "                                                         'anyhow', 'anyone',\n",
       "                                                         'anything', 'anyway', ...})]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tfid_rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf__max_depth': 2,\n",
       " 'rf__max_features': 'auto',\n",
       " 'rf__n_estimators': 200,\n",
       " 'rf__random_state': 42,\n",
       " 'tfid__max_df': 0.9,\n",
       " 'tfid__max_features': 1600,\n",
       " 'tfid__min_df': 2,\n",
       " 'tfid__ngram_range': (1, 2),\n",
       " 'tfid__stop_words': frozenset({'a',\n",
       "            'about',\n",
       "            'above',\n",
       "            'across',\n",
       "            'after',\n",
       "            'afterwards',\n",
       "            'again',\n",
       "            'against',\n",
       "            'all',\n",
       "            'almost',\n",
       "            'alone',\n",
       "            'along',\n",
       "            'already',\n",
       "            'also',\n",
       "            'although',\n",
       "            'always',\n",
       "            'am',\n",
       "            'among',\n",
       "            'amongst',\n",
       "            'amoungst',\n",
       "            'amount',\n",
       "            'amp',\n",
       "            'an',\n",
       "            'and',\n",
       "            'another',\n",
       "            'any',\n",
       "            'anyhow',\n",
       "            'anyone',\n",
       "            'anything',\n",
       "            'anyway',\n",
       "            'anywhere',\n",
       "            'are',\n",
       "            'around',\n",
       "            'as',\n",
       "            'at',\n",
       "            'back',\n",
       "            'be',\n",
       "            'became',\n",
       "            'because',\n",
       "            'become',\n",
       "            'becomes',\n",
       "            'becoming',\n",
       "            'been',\n",
       "            'before',\n",
       "            'beforehand',\n",
       "            'behind',\n",
       "            'being',\n",
       "            'below',\n",
       "            'beside',\n",
       "            'besides',\n",
       "            'between',\n",
       "            'beyond',\n",
       "            'bill',\n",
       "            'both',\n",
       "            'bottom',\n",
       "            'but',\n",
       "            'by',\n",
       "            'call',\n",
       "            'can',\n",
       "            'cannot',\n",
       "            'cant',\n",
       "            'co',\n",
       "            'con',\n",
       "            'could',\n",
       "            'couldnt',\n",
       "            'cry',\n",
       "            'de',\n",
       "            'describe',\n",
       "            'detail',\n",
       "            'do',\n",
       "            'doesn',\n",
       "            'don',\n",
       "            'done',\n",
       "            'down',\n",
       "            'due',\n",
       "            'during',\n",
       "            'each',\n",
       "            'eg',\n",
       "            'eight',\n",
       "            'either',\n",
       "            'eleven',\n",
       "            'else',\n",
       "            'elsewhere',\n",
       "            'empty',\n",
       "            'enough',\n",
       "            'etc',\n",
       "            'even',\n",
       "            'ever',\n",
       "            'every',\n",
       "            'everyone',\n",
       "            'everything',\n",
       "            'everywhere',\n",
       "            'except',\n",
       "            'few',\n",
       "            'fifteen',\n",
       "            'fifty',\n",
       "            'fill',\n",
       "            'find',\n",
       "            'fire',\n",
       "            'first',\n",
       "            'five',\n",
       "            'for',\n",
       "            'former',\n",
       "            'formerly',\n",
       "            'forty',\n",
       "            'found',\n",
       "            'four',\n",
       "            'from',\n",
       "            'front',\n",
       "            'full',\n",
       "            'further',\n",
       "            'get',\n",
       "            'getting',\n",
       "            'give',\n",
       "            'go',\n",
       "            'going',\n",
       "            'got',\n",
       "            'ha',\n",
       "            'had',\n",
       "            'has',\n",
       "            'hasnt',\n",
       "            'have',\n",
       "            'he',\n",
       "            'hence',\n",
       "            'her',\n",
       "            'here',\n",
       "            'hereafter',\n",
       "            'hereby',\n",
       "            'herein',\n",
       "            'hereupon',\n",
       "            'hers',\n",
       "            'herself',\n",
       "            'him',\n",
       "            'himself',\n",
       "            'his',\n",
       "            'how',\n",
       "            'however',\n",
       "            'https',\n",
       "            'hundred',\n",
       "            'i',\n",
       "            'i m',\n",
       "            'ie',\n",
       "            'if',\n",
       "            'in',\n",
       "            'inc',\n",
       "            'indeed',\n",
       "            'interest',\n",
       "            'into',\n",
       "            'is',\n",
       "            'isn',\n",
       "            'it',\n",
       "            'its',\n",
       "            'itself',\n",
       "            'keep',\n",
       "            'last',\n",
       "            'latter',\n",
       "            'latterly',\n",
       "            'least',\n",
       "            'less',\n",
       "            'ltd',\n",
       "            'made',\n",
       "            'many',\n",
       "            'may',\n",
       "            'me',\n",
       "            'meanwhile',\n",
       "            'might',\n",
       "            'mill',\n",
       "            'mine',\n",
       "            'more',\n",
       "            'moreover',\n",
       "            'most',\n",
       "            'mostly',\n",
       "            'move',\n",
       "            'much',\n",
       "            'must',\n",
       "            'my',\n",
       "            'myself',\n",
       "            'name',\n",
       "            'namely',\n",
       "            'neither',\n",
       "            'never',\n",
       "            'nevertheless',\n",
       "            'next',\n",
       "            'nine',\n",
       "            'no',\n",
       "            'nobody',\n",
       "            'none',\n",
       "            'noone',\n",
       "            'nor',\n",
       "            'not',\n",
       "            'nothing',\n",
       "            'now',\n",
       "            'nowhere',\n",
       "            'of',\n",
       "            'off',\n",
       "            'often',\n",
       "            'on',\n",
       "            'once',\n",
       "            'one',\n",
       "            'only',\n",
       "            'onto',\n",
       "            'or',\n",
       "            'other',\n",
       "            'others',\n",
       "            'otherwise',\n",
       "            'our',\n",
       "            'ours',\n",
       "            'ourselves',\n",
       "            'out',\n",
       "            'over',\n",
       "            'own',\n",
       "            'part',\n",
       "            'per',\n",
       "            'perhaps',\n",
       "            'please',\n",
       "            'put',\n",
       "            'rather',\n",
       "            're',\n",
       "            'same',\n",
       "            'see',\n",
       "            'seem',\n",
       "            'seemed',\n",
       "            'seeming',\n",
       "            'seems',\n",
       "            'serious',\n",
       "            'several',\n",
       "            'she',\n",
       "            'should',\n",
       "            'show',\n",
       "            'side',\n",
       "            'since',\n",
       "            'sincere',\n",
       "            'six',\n",
       "            'sixty',\n",
       "            'so',\n",
       "            'some',\n",
       "            'somehow',\n",
       "            'someone',\n",
       "            'something',\n",
       "            'sometime',\n",
       "            'sometimes',\n",
       "            'somewhere',\n",
       "            'still',\n",
       "            'such',\n",
       "            'system',\n",
       "            'take',\n",
       "            'ten',\n",
       "            'than',\n",
       "            'that',\n",
       "            'the',\n",
       "            'their',\n",
       "            'them',\n",
       "            'themselves',\n",
       "            'then',\n",
       "            'thence',\n",
       "            'there',\n",
       "            'thereafter',\n",
       "            'thereby',\n",
       "            'therefore',\n",
       "            'therein',\n",
       "            'thereupon',\n",
       "            'these',\n",
       "            'they',\n",
       "            'thick',\n",
       "            'thin',\n",
       "            'third',\n",
       "            'this',\n",
       "            'those',\n",
       "            'though',\n",
       "            'three',\n",
       "            'through',\n",
       "            'throughout',\n",
       "            'thru',\n",
       "            'thus',\n",
       "            'to',\n",
       "            'together',\n",
       "            'too',\n",
       "            'top',\n",
       "            'toward',\n",
       "            'towards',\n",
       "            'twelve',\n",
       "            'twenty',\n",
       "            'two',\n",
       "            'un',\n",
       "            'under',\n",
       "            'until',\n",
       "            'up',\n",
       "            'upon',\n",
       "            'us',\n",
       "            've',\n",
       "            'very',\n",
       "            'via',\n",
       "            'wa',\n",
       "            'was',\n",
       "            'we',\n",
       "            'well',\n",
       "            'were',\n",
       "            'what',\n",
       "            'whatever',\n",
       "            'when',\n",
       "            'whence',\n",
       "            'whenever',\n",
       "            'where',\n",
       "            'whereafter',\n",
       "            'whereas',\n",
       "            'whereby',\n",
       "            'wherein',\n",
       "            'whereupon',\n",
       "            'wherever',\n",
       "            'whether',\n",
       "            'which',\n",
       "            'while',\n",
       "            'whither',\n",
       "            'who',\n",
       "            'whoever',\n",
       "            'whole',\n",
       "            'whom',\n",
       "            'whose',\n",
       "            'why',\n",
       "            'will',\n",
       "            'with',\n",
       "            'within',\n",
       "            'without',\n",
       "            'would',\n",
       "            'yet',\n",
       "            'you',\n",
       "            'your',\n",
       "            'yours',\n",
       "            'yourself',\n",
       "            'yourselves'})}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tfid_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9598527530246235"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tfid_rf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.964524765729585"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tfid_rf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9698795180722891"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tfid_rf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_tfid__max_df</th>\n",
       "      <th>param_tfid__max_features</th>\n",
       "      <th>param_tfid__min_df</th>\n",
       "      <th>param_tfid__ngram_range</th>\n",
       "      <th>param_rf__max_depth</th>\n",
       "      <th>param_rf__max_features</th>\n",
       "      <th>param_rf__n_estimators</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1600</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>2</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>0.959853</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1200</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>150</td>\n",
       "      <td>0.958510</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1200</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>0.958510</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1200</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>150</td>\n",
       "      <td>0.957842</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1200</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>100</td>\n",
       "      <td>0.957842</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1200</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.908978</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.901616</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.884847</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1</td>\n",
       "      <td>log2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.864118</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.821950</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    param_tfid__max_df param_tfid__max_features param_tfid__min_df  \\\n",
       "34                 0.9                     1600                  2   \n",
       "111                0.9                     1200                  2   \n",
       "114                0.9                     1200                  2   \n",
       "84                 0.9                     1200                  2   \n",
       "108                0.9                     1200                  2   \n",
       "..                 ...                      ...                ...   \n",
       "9                  0.9                     1200                  2   \n",
       "38                 0.9                     2000                  2   \n",
       "17                 0.9                     2000                  2   \n",
       "14                 0.9                     2000                  2   \n",
       "11                 0.9                     2000                  2   \n",
       "\n",
       "    param_tfid__ngram_range param_rf__max_depth param_rf__max_features  \\\n",
       "34                   (1, 2)                   2                   auto   \n",
       "111                  (1, 2)                   5                   auto   \n",
       "114                  (1, 2)                   5                   auto   \n",
       "84                   (1, 2)                   4                   auto   \n",
       "108                  (1, 2)                   5                   auto   \n",
       "..                      ...                 ...                    ...   \n",
       "9                    (1, 2)                   1                   log2   \n",
       "38                   (1, 2)                   2                   log2   \n",
       "17                   (1, 2)                   1                   log2   \n",
       "14                   (1, 2)                   1                   log2   \n",
       "11                   (1, 2)                   1                   log2   \n",
       "\n",
       "    param_rf__n_estimators  mean_test_score  rank_test_score  \n",
       "34                     200         0.959853                1  \n",
       "111                    150         0.958510                2  \n",
       "114                    200         0.958510                2  \n",
       "84                     150         0.957842                4  \n",
       "108                    100         0.957842                4  \n",
       "..                     ...              ...              ...  \n",
       "9                      100         0.908978              131  \n",
       "38                     100         0.901616              132  \n",
       "17                     200         0.884847              133  \n",
       "14                     150         0.864118              134  \n",
       "11                     100         0.821950              135  \n",
       "\n",
       "[135 rows x 9 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a dataframe to show the best rank score from the different params\n",
    "score6_df = pd.DataFrame(gs_tfid_rf.cv_results_).sort_values('rank_test_score')\n",
    "\n",
    "score6_df = score6_df[['param_tfid__max_df', 'param_tfid__max_features', 'param_tfid__min_df',\n",
    "       'param_tfid__ngram_range','param_rf__max_depth', 'param_rf__max_features',\n",
    "       'param_rf__n_estimators', 'mean_test_score','rank_test_score']]\n",
    "score6_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 243\n",
      "False Positives: 5\n",
      "False Negatives: 10\n",
      "True Positives: 240\n",
      "\n",
      "Accuracy: 0.9698795180722891\n",
      "Misclassification: 0.030120481927710885\n",
      "Precision: 0.9795918367346939\n",
      "Sensitivity: 0.96\n",
      "Specificity: 0.9798387096774194\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_matrix(X_test,gs_tfid_rf,'Greens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- Accuracy: 0.9698\n",
    "- True Negative: 243\n",
    "- False Negative: 10\n",
    "- Specificity: 0.9798\n",
    "- Sensitivity: 0.96\n",
    "\n",
    "The training and test accuracy scores for this model are 96.45%, 96.98% respectively indicating that the model is well fitted as the scores are very similar. The true negative count and specificity score are decently well at 243 and 97.98%.\n",
    "Pairing it with low False negative count and high sensitivity score at 10 and 96%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Summary \n",
    "\n",
    "|                   Model |       Transformer | Training Score | Test Score | Specificity | Sensitivity | True Negative | False Positive | False Negative | True Positive |\n",
    "|------------------------:|------------------:|---------------:|-----------:|------------:|------------:|--------------:|---------------:|---------------:|--------------:|\n",
    "|                Baseline |                 - |         0.5020 |          - |           - |           - |             - |              - |              - |             - |\n",
    "| Multinomial Naive Bayes |   CountVectorizer |         0.9692 |     0.9598 |      0.9556 |      0.9640 |           237 |             11 |              9 |           241 |\n",
    "| Multinomial Naive Bayes | TF-IDF Vectorizer |         0.9678 |     0.9538 |      0.9435 |      0.9640 |           234 |             14 |              9 |           241 |\n",
    "|       KNearestNeighbors |   CountVectorizer |         0.8755 |     0.8112 |      0.9717 |      0.6520 |           241 |              7 |             87 |           163 |\n",
    "|       KNearestNeighbors | TF-IDF Vectorizer |            1.0 |     0.9236 |      0.9153 |      0.9320 |           227 |             21 |             17 |           233 |\n",
    "|           Random Forest |   CountVectorizer |         0.9638 |     0.9658 |      0.9596 |      0.9720 |           238 |             10 |              7 |           243 |\n",
    "|           Random Forest | TF-IDF Vectorizer |         0.9645 |     0.9698 |      0.9798 |      0.9600 |           243 |              5 |             10 |           240 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "\n",
    "To revisit what our main goals/concerns are:\n",
    "\n",
    "- High accuracy of classification\n",
    "- Increase the True Negative count.\n",
    "- Reduce False Negative (Type II Error).\n",
    "- Increase Specificity score.\n",
    "- Increase Sensitivity score.\n",
    "\n",
    "My model of choice would be the Second last model, Random Forest paired with Countvectorizer. Reason being is that \n",
    "even though the last model (Random forest with TF-IDF Vectorizer) had achieve slightly better scores on some aspects, The chosen model had better scores on *True Positive count* ,*False Negative count* and *sensitivity score*.\n",
    "\n",
    " - A higher *True Positive count* would dictate that more Keto posts are predicted correctly. \n",
    " - A lower *False Negative count* would dictate that lesser predicted Keto post to be Vegan.\n",
    " - A higher *Sensitivity score* would dictate accuracy of Keto posts that were correctly identified. \n",
    "\n",
    "To refresh our memory:\n",
    "\n",
    "These scores are crucial as ethical vegans have strong beliefs that all creatures have the right to life and freedom. Therefore, they oppose ending a conscious being's life simply to consume its flesh, drink its milk, or wear its skin ‚Äî especially because alternatives are available. And people who are into Keto do not have any concern where they get their nutrition from, be it plants or animals as long as their nutrition requirement has been fulfilled. Therefore we take the mentioned scores into serious consideration to mitigate the risk of Keto posts being classified as Vegan for our client.   \n",
    "\n",
    "However, as there are still room for improvement in the model that we have explored. We can further improve our model with more data (posts being scrapped) or even increase the time horizon of the data scrapped to probably 2-3 years post's age. This is because forums usually have lingos so we might be able to capture such lingos used in different subreddit to help increase the accuracy. We can also explore using boosting in models such as XGBoost to see if they yield better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
